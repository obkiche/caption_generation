{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0e4975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras import Input, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cc5fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath= './data/txt_data'\n",
    "infile = os.path.join(mypath, 'Flickr8k.token.txt')\n",
    "print(infile)\n",
    "\n",
    "#outfile = os.path.join(mypath, 'Flickr8kmod.token.txt')\n",
    "'''\n",
    "fout = open(outfile, \"w\")\n",
    "with open(infile) as fin:\n",
    "    lines = fin.readlines()\n",
    "    count = 0\n",
    "    for line in lines:\n",
    "        line = line.replace('#', '\\t',1)\n",
    "        \n",
    "        fout.write(line)\n",
    "'''       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828cb6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile = os.path.join(mypath, 'Flickr8kmod.token.txt')\n",
    "df_raw = pd.read_csv(myfile, '\\t',header=None)\n",
    "df_raw.columns = ['image_id', 'label', 'caption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec47940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_doc(filename):\n",
    "    \n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def load_descriptions(doc):\n",
    "    \n",
    "    mydic = {}\n",
    "    for line in doc.split('\\n'):\n",
    "        \n",
    "        tokens = line.split()\n",
    "        if len(tokens) < 2:\n",
    "            continue\n",
    "            \n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        image_id = image_id.split('.')[0]\n",
    "        image_desc = ' '.join(image_desc)\n",
    "        \n",
    "        if image_id not in mydic:\n",
    "            mydic[image_id] = []\n",
    "        mydic[image_id].append(image_desc)\n",
    "        \n",
    "    \n",
    "    return mydic\n",
    "\n",
    "doc = read_doc(infile)\n",
    "descriptions = load_descriptions(doc)\n",
    "print('Loaded: %d ' % len(descriptions))\n",
    "print(list(descriptions.items())[:5])\n",
    "descriptions['1000268201_693b08cb0e'][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0e972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_descriptions(descriptions):\n",
    "    clean_desc = {}\n",
    "    for key, desc_list in descriptions.items():\n",
    "        \n",
    "        for i in range(len(desc_list)):\n",
    "        \n",
    "            desc = desc_list[i]\n",
    "            desc = desc.lower()        \n",
    "            desc = re.sub(\"[^a-z]+\",\" \", desc)        \n",
    "            desc = desc.split()\n",
    "            desc = [word for word in desc if len(word) > 1]\n",
    "        \n",
    "            desc =  ' '.join(desc)\n",
    "            \n",
    "            if key not in clean_desc:\n",
    "                clean_desc[key] = []\n",
    "            clean_desc[key].append(desc)\n",
    "        \n",
    "     \n",
    "    return clean_desc\n",
    "    \n",
    "            \n",
    "\n",
    "clean_desc = clean_descriptions(descriptions)\n",
    "print('***********')\n",
    "clean_desc['1000268201_693b08cb0e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece90b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = set()\n",
    "\n",
    "for key in clean_desc.keys():\n",
    "    \n",
    "    for sentance in clean_desc[key]: \n",
    "        \n",
    "        all_words.update(sentance.split()) \n",
    "print((all_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c62ff2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e37145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_descriptions(description, filename):\n",
    "    lines = list()\n",
    "    for key, desc_list in description.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + \" \" + desc)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68769c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_descriptions(clean_desc, os.path.join(mypath,'clean_descriptions.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2909d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_set(filename):\n",
    "    \n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "   \n",
    "    dataset = list()\n",
    "    for line in text.split('\\n'):\n",
    "        if(len(line)) < 1:\n",
    "            continue\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)\n",
    "\n",
    "inf = os.path.join(mypath, 'Flickr_8k.trainImages.txt')\n",
    "\n",
    "train = load_set(inf)\n",
    "print(len(train))\n",
    "print(list(train)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c2a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "img_path = './data/flicker_dataset/Flicker8k_Dataset/'\n",
    "print(img_path)\n",
    "img = glob.glob(img_path+'*.jpg')\n",
    "img[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd840e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_path = './data/txt_data/Flickr_8k.trainImages.txt'\n",
    "train_img = set(open(train_img_path, 'r').read().strip().split('\\n'))\n",
    "print(train_img)\n",
    "train_images = []\n",
    "\n",
    "for i in img:\n",
    "    if i[len(img_path):] in train_img:\n",
    "        train_images.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76737d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279a6ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_description(filename, dataset):\n",
    "    \n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    descriptions = dict()\n",
    "    for line in text.split('\\n'):\n",
    "        tokens = line.split()\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        if image_id in dataset:\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            desc = 'startseq '+' '.join(image_desc) + ' endseq'\n",
    "            descriptions[image_id].append(desc)\n",
    "    return descriptions\n",
    "cleaned_desc = os.path.join(mypath,'clean_descriptions.txt')\n",
    "train_description = load_clean_description(cleaned_desc, train)\n",
    "len(train_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ea50e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_list(descriptions):\n",
    "    all_desc = []\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "#creating tokenizer class \n",
    "#this will vectorise text corpus\n",
    "#each integer will represent token in dictionary\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "def create_tokenizer(descriptions):\n",
    "    desc_list = dict_to_list(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(desc_list)\n",
    "    return tokenizer\n",
    "# give each word an index, and store that into tokenizer.p pickle file\n",
    "tokenizer = create_tokenizer(train_description)\n",
    "dump(tokenizer, open('tokenizer.p', 'wb'))\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfe580d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train our model\n",
    "print('Dataset: ', len(train_img))\n",
    "print('Descriptions: train=', len(train_description))\n",
    "print('Photos: train=', len(train_features))\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Description Length: ', max_length)\n",
    "model = define_model(vocab_size, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80e5386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate maximum length of descriptions\n",
    "def max_length(descriptions):\n",
    "    desc_list = dict_to_list(descriptions)\n",
    "    return max(len(d.split()) for d in desc_list)\n",
    "    \n",
    "max_length = max_length(descriptions)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96179bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = load(open(\"features.p\",\"rb\"))\n",
    "train_img_path = './data/txt_data/Flickr_8k.trainImages.txt'\n",
    "img_path = './data/flicker_dataset/Flicker8k_Dataset/'\n",
    "train_img = set(open(train_img_path, 'r').read().strip().split('\\n'))\n",
    "\n",
    "train_features = {k:features[k] for k in train_img}\n",
    "\n",
    "print(list(train_features)[:3])\n",
    "print(list(train_description)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a508a21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(descriptions, features, tokenizer, max_length, batch_size):\n",
    "    n = 0\n",
    "    for key, description_list in descriptions.items():\n",
    "        n+=1\n",
    "        #retrieve photo features\n",
    "        key1 = key + '.jpg'\n",
    "        feature = features[key1][0]\n",
    "        input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list, feature)\n",
    "        if n==batch_size:\n",
    "            yield ([input_image, input_sequence], output_word)\n",
    "            X1,X2,y = [],[],[]\n",
    "            n = 0\n",
    "            \n",
    "def create_sequences(tokenizer, max_length, desc_list, feature):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    # walk through each description for the image\n",
    "    for desc in desc_list:\n",
    "        # encode the sequence\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        # split one sequence into multiple X,y pairs\n",
    "        for i in range(1, len(seq)):\n",
    "            # split into input and output pair\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            # pad input sequence\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            # encode output sequence\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            # store\n",
    "            X1.append(feature)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "            \n",
    "    return np.array(X1), np.array(X2), np.array(y)\n",
    "#You can check the shape of the input and output for your model\n",
    "[a,b],c = next(data_generator(train_description, train_features, tokenizer, max_length, 3))\n",
    "a.shape, b.shape, c.shape\n",
    "#((47, 2048), (47, 32), (47, 7577))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2e5ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_description.values()\n",
    "#seq = tokenizer.texts_to_sequences([train_description.values()[1]])[0]\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb91904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
    "                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.merge import add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Input, layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#from keras.utils import plot_model\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c3c64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "    # features from the CNN model squeezed from 2048 to 256 nodes\n",
    "    inputs1 = Input(shape=(1000,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    # LSTM sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "    # Merging both models\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=(inputs1, inputs2), outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    # summarize model\n",
    "    print(model.summary())\n",
    "    #plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112b62d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs = 10\n",
    "batch_size = 6\n",
    "#model.optimizer.lr = 0.0001\n",
    "number_pics_per_batch = 6\n",
    "#steps = len(train_description)\n",
    "\n",
    "steps = len(train_description)//number_pics_per_batch\n",
    "# making a directory models to save our models\n",
    "#os.mkdir(\"models\")\n",
    "generator = data_generator(train_description, train_features, tokenizer, max_length, batch_size)\n",
    "for i in range(epochs):\n",
    "    model.fit(generator, epochs=6, steps_per_epoch=steps, verbose=1)\n",
    "    model.save(\"models/model_\" + str(i) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970dfca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "epochs = 10\n",
    "number_pics_per_bath = 6\n",
    "steps = len(train_description)//number_pics_per_bath\n",
    "steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fb3cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    generator = data_generator(train_description, train_features, tokenizer, max_length)\n",
    "    model.fit(generator, epochs=6, steps_per_epoch=steps, verbose=1)\n",
    "    model.save('./model_weights/model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30556643",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
